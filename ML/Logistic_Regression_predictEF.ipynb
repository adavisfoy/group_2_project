{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5805e074",
   "metadata": {},
   "source": [
    "MOCK-UP OF GROUP 2 SUPERVISED ML MODEL: Logistic REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bdcb8cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import dependencies\n",
    "import pandas as pd \n",
    "from pathlib import Path \n",
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a3233e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load tornado data CSV as a Pandas DataFrame and preview the DataFrame\n",
    "# file_path = Path('../Cleaned_Data/2008-2020_tornadoes_EF_cleaned.csv')\n",
    "# df = pd.read_csv(file_path, index_col=0)\n",
    "# df.head(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9427e0e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'apt-get' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "The system cannot find the path specified.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "tar: Error opening archive: Failed to open '$SPARK_VERSION-bin-hadoop2.7.tgz'\n"
     ]
    }
   ],
   "source": [
    "# Apache Spark Set Up\n",
    "\n",
    "import os\n",
    "# Find the latest version of spark 3.0 from http://www.apache.org/dist/spark/ and enter as the spark version\n",
    "# For example:\n",
    "# spark_version = 'spark-3.0.3'\n",
    "spark_version = 'spark-3.2.1'\n",
    "os.environ['SPARK_VERSION']=spark_version\n",
    "\n",
    "# Install Spark and Java\n",
    "!apt-get update\n",
    "!apt-get install openjdk-11-jdk-headless -qq > /dev/null\n",
    "!wget -q http://www.apache.org/dist/spark/$SPARK_VERSION/$SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!tar xf $SPARK_VERSION-bin-hadoop2.7.tgz\n",
    "!pip install -q findspark\n",
    "\n",
    "# Set Environment Variables\n",
    "import os\n",
    "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-11-openjdk-amd64\"\n",
    "os.environ[\"SPARK_HOME\"] = f\"/content/{spark_version}-bin-hadoop2.7\"\n",
    "\n",
    "# Start a SparkSession\n",
    "import findspark\n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5599be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start Spark session\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Tornadoes\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a8479",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in data from S3 Buckets\n",
    "from pyspark import SparkFiles\n",
    "url =\"https://group-2-project-tornadoes.s3.amazonaws.com/2008-2020_tornadoes_EF_cleaned_db.csv\"\n",
    "spark.sparkContext.addFile(url)\n",
    "tornado = spark.read.csv(SparkFiles.get(\"2008-2020_tornadoes_EF_cleaned_db.csv\"), sep=\",\", header=True)\n",
    "\n",
    "# Show DataFrame\n",
    "tornado.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1bdbadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert spark object to pandas dataframe\n",
    "tornado_df = tornado.toPandas()\n",
    "tornado_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e74d240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "tornado_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06c16a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert timestamp to datetime\n",
    "tornado_df['Timestamp']= pd.to_datetime(tornado_df['Timestamp'])\n",
    "\n",
    "# Convert all strings to numeric\n",
    "cols=[i for i in tornado_df.columns if i not in [\"Timestamp\"]]\n",
    "for col in cols:\n",
    "    tornado_df[col]=pd.to_numeric(tornado_df[col])\n",
    "\n",
    "# check dtypes\n",
    "tornado_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fc7d5c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Review columns\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a368a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually inspect relationship between tornado magnitude ('EF' column) and 'Loss' - positive trend\n",
    "plt.scatter(df.EF, df.Loss) \n",
    "plt.xlabel('Tornado Magnitude') \n",
    "plt.ylabel('Loss') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1baf02a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually inspect relationship between tornado magnitude ('EF' column) and 'Length' - positive trend\n",
    "plt.scatter(df.EF, df.Length) \n",
    "plt.xlabel('Tornado Magnitude') \n",
    "plt.ylabel('Length') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a707cdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visually inspect relationship between tornado magnitude ('EF' column) and 'Width' - positive trend\n",
    "plt.scatter(df.EF, df.Width) \n",
    "plt.xlabel('Tornado Magnitude') \n",
    "plt.ylabel('Width') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfc50f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format data to meet the requirements of the Scikit-learn library: \n",
    "# Will add Loss later\n",
    "feature_names = [\"Length\", \"Width\"]\n",
    "X = df[feature_names]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4483f26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign the target variable - EF\n",
    "y = df.EF.values.reshape(-1, 1)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6b0f3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c96f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e833bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Logistic Regression\n",
    "\n",
    "# Our data is not linear - Reject model\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train_scaled, y_train)\n",
    "\n",
    "print('Accuracy of logistic regression on training', logreg.score(X_train_scaled, y_train))\n",
    "print('Accuracy of logistic regression on testing', logreg.score(X_test_scaled, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88007f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "mlenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
